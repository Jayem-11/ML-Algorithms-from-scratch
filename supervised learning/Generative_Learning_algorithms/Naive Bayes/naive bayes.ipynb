{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f426a4",
   "metadata": {},
   "source": [
    "# B. Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10919c2",
   "metadata": {},
   "source": [
    "In Naive Bayes, the input features are discrete valued. This means, they are either $0$ or $1$.Our main objective is creating a model that can classify whethee the input is $o$ or $1$.\n",
    "\n",
    "For example, we can assume that we want to build a model that can classifiy whether a movie review is whether a movie review is positive($1$) or negative($0$).\n",
    "\n",
    "In the case of a movie review, the input will be a statement like \"The movie was good\". We need to convert this statement to a vector. For this we need a vocabulary of words.\n",
    "\n",
    "Let our vocabulary of word be a dictionary conataining 5000 words. We can express the example \"The movie was good\" as follows:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix} \n",
    "    0 \\\\ \n",
    "    1 & (good) \\\\ \n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1 & (movie)\\\\\n",
    "    0 \\\\\n",
    "    1 & (The)\\\\\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1 & (was)\\\\\n",
    "    0 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, the length of the feature length is the same as the number of vocabularies in the dictionary. Words present in the dictionary that a apprear in the input, are represented by $1$ whereas, words that in the dictionary that do not appear in the input are represented by $0$ as shown above.\n",
    "\n",
    "\n",
    "### Model  \n",
    "\n",
    "\n",
    "Since both the input features and the label are discrete, we make an assumption that:\n",
    "\n",
    "$P(y = 1) \\thicksim Bernoulli( \\phi_y )$    \n",
    "The probability that the movie review is positive $(1)$ follows a Bernoulli Distribution\n",
    "\n",
    "$P(x_j =1 | y = 0) \\thicksim Bernoulli( \\phi_{j | y = 0} )$  \n",
    "The probability of the input features follows a Bernoulli distibution given that the label is a negative review\n",
    "  \n",
    "$P(x_j =1 | y = 1) \\thicksim Bernoulli( \\phi_{j | y = 1} )$  \n",
    "The probability of the input feature follows a Bernoulli distibution given that the label is a positive review\n",
    "\n",
    "\n",
    "Having chosen our feature vector, we have to build our generative model. In order to reduce the parameter size of input features, we make a strong assumption. We assume that $x_i's$ are conditionally independent given y. This assuption is called **Naive Bayes** assumption. The assumption means that the input features are only independent for a particular output. \n",
    "\n",
    "\n",
    "#### Bayes Rule:\n",
    "\n",
    "$$ p(y|x) = {p(x|y)p(y) \\over p(x)} $$\n",
    "\n",
    "We need to find each of the value:  \n",
    "$1. P(x|y)$\n",
    "\n",
    "Due to the conditional independence assumption:\n",
    "\n",
    "$$P(X_1,X_2,...X_{5000}|Y) = P(X_1|y).P(X_2|x_1,y).P(X_3|x_1,x_2,y) .... $$\n",
    "$$ = P(X_1|y).P(X_2|y).P(X_3|y)$$\n",
    "$$ = \\prod^n_{i=1}P(X_i|y)$$\n",
    "We need to calculate this for each of the output label\n",
    "\n",
    "**Joint likelihood**  \n",
    "$$ L(\\phi_y,\\phi_{j | y = 0},  \\phi_{j | y = 1}) = \\prod^n_{i=1}P(x^{(i)},y^{(i)}) $$\n",
    "\n",
    "\n",
    "\n",
    "Maximizing with respect to the parameters results to the following maximum likelihood estimates:\n",
    "\n",
    "$$\\phi_{j | y = 1} = {\\sum^n_{i=1} 1\\{x^{(i)}_j=1 \\land y^{(i)}=1\\} \\over \\sum^n_{i=1} 1\\{y^{(i)}=1\\}}$$\n",
    "\n",
    "$$\\phi_{j | y = 0} = {\\sum^n_{i=1} 1\\{x^{(i)}_j=1 \\land y^{(i)}=0\\} \\over \\sum^n_{i=1} 1\\{y^{(i)}=0\\}}$$\n",
    "\n",
    "\n",
    "$2.P(y)$  \n",
    "This is referred to as the class probability\n",
    "\n",
    "$$\\phi_y = {\\sum^n_{i=1} 1\\{y^{(i)}=1\\} \\over n }$$\n",
    "\n",
    "$3.P(x)$  \n",
    "\n",
    "$$P(x) = \\sum^k_{i=1}\\prod^n_{j=1}P(x_{(i)}|y=k)$$\n",
    "\n",
    "**Prediction**\n",
    "    \n",
    "$$ p(y=k|x) = {p(x|y =k)p(y=k) \\over p(x)} $$\n",
    "\n",
    "where $k \\epsilon \\{0,1\\}$\n",
    "\n",
    "The class with the highest probability, either $0$ or $1$ is considered as the output label. Discretizing continuous values allows for naive bayes classification.\n",
    "\n",
    "## Laplace Smoothing\n",
    "**Problem**  \n",
    "We encounter the following review,\" The movie was really horrible\" and our dictionary of vocabularies does not contain the word horible.\n",
    "\n",
    "$$(x_{horrible}|y) = 0 $$\n",
    "for all values of 'k'\n",
    "\n",
    "$$(x_{horrible}|y) = 0 $$\n",
    "\n",
    "$$p(y|x) = undefined $$\n",
    "\n",
    "**Solution**\n",
    "\n",
    "In order to solve this problem, we use laplace smoothing.\n",
    "\n",
    "$$P(y = k) = {\\sum^n_{i=1} 1\\{y^{(i)}=k\\} + 1 \\over n + m }$$\n",
    "\n",
    "where m is the number of output class. In the movie example m is 2\n",
    "\n",
    "$$P(x_j | y = k)= {\\sum^n_{i=1} 1\\{x^{(i)}_j=1 \\land y^{(i)}=k\\} \\over \\sum^n_{i=1} 1\\{y^{(i)}=k\\} + n}$$\n",
    "\n",
    "\n",
    "where $\"\\land\"$ is the symbol for AND\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
